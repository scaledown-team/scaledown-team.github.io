{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ScaleDown ScaleDown is an Open-Source Neural Network Optimization Framework for TinyML Devices. Simply put, we help you scaledown your large neural networks into small ones so that you can deploy them to resource constrained edge computing devices like Arduino's, Raspberry Pi's and Mobile Phones. If you are wondering what TinyML or Edge Computing is, take a look at our free course Features Broadly to run models on TinyML devices, you need to be able to do the following: - Optimize your model using algorithms like quantization, pruning, knowledge distillation etc., Benchmark and Evaluate your model's performance, and, Deploy your model to the TinyML device. These are the three verticals that ScaleDown takes care of so that you can focus on the main task of getting data and training models for interesting applications! In addition to the above, we also aim to be framework agnostic so you can use our algorithms no matter what deep learning library you used to train your model. We do this by having APIs for model conversion from your framework to whichever framework supports the algorithm you want to optimize your model with. Finally, we know how it can be difficult to properly optimize your models for edge devices, so we will also provide API for automated model optimization based on your hardware restrictions. Optimization Techniques Currently we offer serveral optimization techniques for TinyML. We also offer a wide range of techniques which can help you in your applications. Type of Optimisation Tensorflow OpenVino PyTorch NVIDIA TensorRT Quantization \u2713 In Progress In Progress Will be available soon! Knowledge Distillation \u2713 \u2717 * \u2713 Will be available soon! Pruning In Progress \u2717 * In Progress Will be available soon! Conversion of Models between Frameworks Alongside that, we also offer conversion between frameworks. So in case, you want to work across different devices supporting different frameworks, this wrapper can help convert those models for you. From To Tensorflow To OpenVino To PyTorch ONXX To NVIDIA TensorRT Tensorflow \u2717 * In Progress \u2717 * \u2713 Will be available soon! OpenVino In Progress \u2717 * In Progress In Progress Will be available soon! PyTorch \u2713 In Progress \u2717 * \u2713 Will be available soon! ONNX \u2713 In Progress In Progress \u2717 * Will be available soon! \"*\" = This Optimisation technique is not supported by the frameworks. In case, you find this information misleading, feel free to raise an issue on Github Contact Us ScaleDown is currently in active development. In case you run into any issues, feel free to raise an issue on Github . You can also contact us via email","title":"Home"},{"location":"#scaledown","text":"ScaleDown is an Open-Source Neural Network Optimization Framework for TinyML Devices. Simply put, we help you scaledown your large neural networks into small ones so that you can deploy them to resource constrained edge computing devices like Arduino's, Raspberry Pi's and Mobile Phones. If you are wondering what TinyML or Edge Computing is, take a look at our free course","title":"ScaleDown"},{"location":"#features","text":"Broadly to run models on TinyML devices, you need to be able to do the following: - Optimize your model using algorithms like quantization, pruning, knowledge distillation etc., Benchmark and Evaluate your model's performance, and, Deploy your model to the TinyML device. These are the three verticals that ScaleDown takes care of so that you can focus on the main task of getting data and training models for interesting applications! In addition to the above, we also aim to be framework agnostic so you can use our algorithms no matter what deep learning library you used to train your model. We do this by having APIs for model conversion from your framework to whichever framework supports the algorithm you want to optimize your model with. Finally, we know how it can be difficult to properly optimize your models for edge devices, so we will also provide API for automated model optimization based on your hardware restrictions.","title":"Features"},{"location":"#optimization-techniques","text":"Currently we offer serveral optimization techniques for TinyML. We also offer a wide range of techniques which can help you in your applications. Type of Optimisation Tensorflow OpenVino PyTorch NVIDIA TensorRT Quantization \u2713 In Progress In Progress Will be available soon! Knowledge Distillation \u2713 \u2717 * \u2713 Will be available soon! Pruning In Progress \u2717 * In Progress Will be available soon!","title":"Optimization Techniques"},{"location":"#conversion-of-models-between-frameworks","text":"Alongside that, we also offer conversion between frameworks. So in case, you want to work across different devices supporting different frameworks, this wrapper can help convert those models for you. From To Tensorflow To OpenVino To PyTorch ONXX To NVIDIA TensorRT Tensorflow \u2717 * In Progress \u2717 * \u2713 Will be available soon! OpenVino In Progress \u2717 * In Progress In Progress Will be available soon! PyTorch \u2713 In Progress \u2717 * \u2713 Will be available soon! ONNX \u2713 In Progress In Progress \u2717 * Will be available soon! \"*\" = This Optimisation technique is not supported by the frameworks. In case, you find this information misleading, feel free to raise an issue on Github","title":"Conversion of Models between Frameworks"},{"location":"#contact-us","text":"ScaleDown is currently in active development. In case you run into any issues, feel free to raise an issue on Github . You can also contact us via email","title":"Contact Us"},{"location":"contributeyourproject/","text":"","title":"Contributeyourproject"},{"location":"hardware_library/","text":"Hardware Library The Hardware Library is a physical library present in a city near you. How does it work? The Hardware Library is like any other library, you can use the hardware and then return it. Anyone can use the hardware present in the list below, free of cost. Once they are done experimenting and running their first hardware library, they can return it and get another one, to experiment on. Currently we have started our library in Singapore. List of Hardware Want to start a Hardware Library? Do you want to start a hardware library at your city. Reach out to us Frequently asked Questions How do I know which Hardware to pick? How do I return my hardware to the library?","title":"Hardware Library"},{"location":"hardware_library/#hardware-library","text":"The Hardware Library is a physical library present in a city near you.","title":"Hardware Library"},{"location":"hardware_library/#how-does-it-work","text":"The Hardware Library is like any other library, you can use the hardware and then return it. Anyone can use the hardware present in the list below, free of cost. Once they are done experimenting and running their first hardware library, they can return it and get another one, to experiment on. Currently we have started our library in Singapore.","title":"How does it work?"},{"location":"hardware_library/#list-of-hardware","text":"","title":"List of Hardware"},{"location":"hardware_library/#want-to-start-a-hardware-library","text":"Do you want to start a hardware library at your city. Reach out to us","title":"Want to start a Hardware Library?"},{"location":"hardware_library/#frequently-asked-questions","text":"How do I know which Hardware to pick? How do I return my hardware to the library?","title":"Frequently asked Questions"},{"location":"learn/","text":"Learning TinyML TinyML combines techniques from embedded systems, IoT, software engineering and machine learning. At ScaleDown we understand that learning about these fields can be daunting. This is why we have been working on creating courses, study groups, books and other learning materials to help you learn about this field. Below are some the things we have done and are working on! Study Groups We started our first study group in Feb, 2021. We wanted to teach students about TinyML by telling them about the different TinyML techniques and tools and also showing them how to use those tools to build projects. Moreover, since most new TinyML techniques are still released in the form of research papers, we also taught students how to stay updated in the field by reading and discussing research papers. You can find all the slides and videos of the study group here . Books Practical TinyMLOps : TinyMLOps is MLOps for TinyML. Preparing models for TinyML requires extra steps which a standard MLOps pipeline does not have. Furthermore, many standard MLOps steps need to augmented with TinyML specific implementations or even removed. This is why we need TinyMLOps. In this book we talk about what TinyMLOps is, the different parts of a TinyMLOps system and how to build your own TinyMLOps system. Note that this book is currently a work in progress!","title":"Learning TinyML"},{"location":"learn/#learning-tinyml","text":"TinyML combines techniques from embedded systems, IoT, software engineering and machine learning. At ScaleDown we understand that learning about these fields can be daunting. This is why we have been working on creating courses, study groups, books and other learning materials to help you learn about this field. Below are some the things we have done and are working on!","title":"Learning TinyML"},{"location":"learn/#study-groups","text":"We started our first study group in Feb, 2021. We wanted to teach students about TinyML by telling them about the different TinyML techniques and tools and also showing them how to use those tools to build projects. Moreover, since most new TinyML techniques are still released in the form of research papers, we also taught students how to stay updated in the field by reading and discussing research papers. You can find all the slides and videos of the study group here .","title":"Study Groups"},{"location":"learn/#books","text":"Practical TinyMLOps : TinyMLOps is MLOps for TinyML. Preparing models for TinyML requires extra steps which a standard MLOps pipeline does not have. Furthermore, many standard MLOps steps need to augmented with TinyML specific implementations or even removed. This is why we need TinyMLOps. In this book we talk about what TinyMLOps is, the different parts of a TinyMLOps system and how to build your own TinyMLOps system. Note that this book is currently a work in progress!","title":"Books"},{"location":"team/","text":"Team Team Details here","title":"Team"},{"location":"team/#team","text":"Team Details here","title":"Team"},{"location":"api/","text":"API Reference This is the reference for all the classes and functions available in ScaleDown. We also have a User Guide that contains technical information and implementation details about the algorithms available as a part of this framework. If you want to learn more about TinyML and how it works, check out our free books and videos . scaledown.load_model : Loading Models Utilities for loading supported models to scaledown. This is usually the first step that you need to do when working with scaledown. Once the models are loaded, you can then optimize them, benchmark their performance and then deploy them to different devices. For more information on how to load models and get started with ScaleDown, check out our User Guide . scaledown.load_model(model, model_type) : Wrapper function for loading supported models scaledown.quantization : Quantizing Models Utilities for optimizing models uing quantizing. User Guide : See Quantization to learn more about the supported quantization algorithms and how they work quantization.WeightQuantization : Perform only weight quantization of models. Good for reducing model size with less drop in accuracy. quantization.ActivationQuantization : Perform weight and activation quantization of models. Good for reducing model size and for using INT8 operations for executing models. scaledown.distillation : Distilling Large Models Utilities for distilling the knowledge of large models to smaller ones. User Guide : See Distillation to learn more about the supported knowledge distillation algorithms and how they work distillation.KnowledgeDistillation : Distill models using the original knowledge distillation technique proposed by Hinton et. al. Supports add temperature to distillation as well. scaledown.pruning : Pruning Models Utilities for optimizing models using different pruning techniques. User Guide : See Pruning to learn more about the supported pruning algorithms and how they work pruning.MagnitudePruning : Prune models by removing weights with high magnitude. Support for finetuning model after pruning is present.","title":"API Reference"},{"location":"api/#api-reference","text":"This is the reference for all the classes and functions available in ScaleDown. We also have a User Guide that contains technical information and implementation details about the algorithms available as a part of this framework. If you want to learn more about TinyML and how it works, check out our free books and videos .","title":"API Reference"},{"location":"api/#scaledownload_model-loading-models","text":"Utilities for loading supported models to scaledown. This is usually the first step that you need to do when working with scaledown. Once the models are loaded, you can then optimize them, benchmark their performance and then deploy them to different devices. For more information on how to load models and get started with ScaleDown, check out our User Guide . scaledown.load_model(model, model_type) : Wrapper function for loading supported models","title":"scaledown.load_model: Loading Models"},{"location":"api/#scaledownquantization-quantizing-models","text":"Utilities for optimizing models uing quantizing. User Guide : See Quantization to learn more about the supported quantization algorithms and how they work quantization.WeightQuantization : Perform only weight quantization of models. Good for reducing model size with less drop in accuracy. quantization.ActivationQuantization : Perform weight and activation quantization of models. Good for reducing model size and for using INT8 operations for executing models.","title":"scaledown.quantization: Quantizing Models"},{"location":"api/#scaledowndistillation-distilling-large-models","text":"Utilities for distilling the knowledge of large models to smaller ones. User Guide : See Distillation to learn more about the supported knowledge distillation algorithms and how they work distillation.KnowledgeDistillation : Distill models using the original knowledge distillation technique proposed by Hinton et. al. Supports add temperature to distillation as well.","title":"scaledown.distillation: Distilling Large Models"},{"location":"api/#scaledownpruning-pruning-models","text":"Utilities for optimizing models using different pruning techniques. User Guide : See Pruning to learn more about the supported pruning algorithms and how they work pruning.MagnitudePruning : Prune models by removing weights with high magnitude. Support for finetuning model after pruning is present.","title":"scaledown.pruning: Pruning Models"},{"location":"api/knowledge_distillation/","text":"scaledown.distillation.KnowledgeDistillation class scaledown . quantization . KnowledgeDistillation ( teacher , student , optimizer , distillation_loss , student_loss , metric , activation , temperature = 1 ) Parameters teacher : model Specify the teacher model you want to perform distillation with. Should be a trained model. student : model Specify the student model you want to distill the knowledge to. optimizer : tf or pytorch optimizer The optimizer you want to use to perform distillation distillation_loss : tf or pytorch loss The loss function you want to use to perform distillation student_loss : tf or pytorch loss The loss function you want to evaluate the student model with metric : tf or pytorch metric The metric you want to use to evaluate your model with activation : tf or pytorch activation The activation function you want to use at the end of your student model. This is not needed if an activation is already specified temperature : int The temperature to use while performing distillation Methods train_step(data) : Takes a training step for the data provided.","title":"scaledown.distillation.KnowledgeDistillation"},{"location":"api/knowledge_distillation/#scaledowndistillationknowledgedistillation","text":"class scaledown . quantization . KnowledgeDistillation ( teacher , student , optimizer , distillation_loss , student_loss , metric , activation , temperature = 1 )","title":"scaledown.distillation.KnowledgeDistillation"},{"location":"api/knowledge_distillation/#parameters","text":"teacher : model Specify the teacher model you want to perform distillation with. Should be a trained model. student : model Specify the student model you want to distill the knowledge to. optimizer : tf or pytorch optimizer The optimizer you want to use to perform distillation distillation_loss : tf or pytorch loss The loss function you want to use to perform distillation student_loss : tf or pytorch loss The loss function you want to evaluate the student model with metric : tf or pytorch metric The metric you want to use to evaluate your model with activation : tf or pytorch activation The activation function you want to use at the end of your student model. This is not needed if an activation is already specified temperature : int The temperature to use while performing distillation","title":"Parameters"},{"location":"api/knowledge_distillation/#methods","text":"train_step(data) : Takes a training step for the data provided.","title":"Methods"},{"location":"api/magnitude_pruning/","text":"","title":"Magnitude pruning"},{"location":"api/weight_quantization/","text":"scaledown.quantization.weight_quantization class scaledown.quantization. weight_quantization (level='float16') Parameters level : str, ['float16', 'int8'] Used for setting the level of quantization Attributes quantized_model : Returns the quantized model. Methods scaledown","title":"scaledown.quantization.weight\\_quantization"},{"location":"api/weight_quantization/#scaledownquantizationweight_quantization","text":"class scaledown.quantization. weight_quantization (level='float16')","title":"scaledown.quantization.weight_quantization"},{"location":"api/weight_quantization/#parameters","text":"level : str, ['float16', 'int8'] Used for setting the level of quantization","title":"Parameters"},{"location":"api/weight_quantization/#attributes","text":"quantized_model : Returns the quantized model.","title":"Attributes"},{"location":"api/weight_quantization/#methods","text":"scaledown","title":"Methods"},{"location":"conferences/fossasia/","text":"FOSS Asia Thank you for attending our lightning talk! Want to know more about ScaleDown? Check out a Short Demo Check out this short video and watch how ScaleDown can be implemented: How is ScaleDown Helping in quicker implementation? ScaleDown makes it easy to train and deploy models by giving you wrappers over commonly used TinyML algorithms and techniques. ScaleDown also handles the implementation details and manages hardware-software interoperability making it easy to develop TinyML projects. Check out our how ScaleDown simplifies complex steps during a TinyML cycle. Stay Updated We are constantly looking to ramp our work and open-source package, sign up to get updates on new optimizations and framework adoptions. Sign Up for Updates","title":"FOSS Asia"},{"location":"conferences/fossasia/#foss-asia","text":"Thank you for attending our lightning talk! Want to know more about ScaleDown?","title":"FOSS Asia"},{"location":"conferences/fossasia/#check-out-a-short-demo","text":"Check out this short video and watch how ScaleDown can be implemented:","title":"Check out a Short Demo"},{"location":"conferences/fossasia/#how-is-scaledown-helping-in-quicker-implementation","text":"ScaleDown makes it easy to train and deploy models by giving you wrappers over commonly used TinyML algorithms and techniques. ScaleDown also handles the implementation details and manages hardware-software interoperability making it easy to develop TinyML projects. Check out our how ScaleDown simplifies complex steps during a TinyML cycle.","title":"How is ScaleDown Helping in quicker implementation?"},{"location":"conferences/fossasia/#stay-updated","text":"We are constantly looking to ramp our work and open-source package, sign up to get updates on new optimizations and framework adoptions. Sign Up for Updates","title":"Stay Updated"},{"location":"conferences/tinymlsummit/","text":"TinyML Summit Thank you for stopping by our Poster! Want to know more about ScaleDown? Check out a Short Demo Check out this short video and watch how ScaleDown can be implemented: How is ScaleDown Helping in quicker implementation? ScaleDown makes it easy to train and deploy models by giving you wrappers over commonly used TinyML algorithms and techniques. ScaleDown also handles the implementation details and manages hardware-software interoperability making it easy to develop TinyML projects. Check out our how ScaleDown simplifies complex steps during a TinyML cycle. Stay Updated We are constantly looking to ramp our work and open-source package, sign up to get updates on new optimizations and framework adoptions. Sign Up for Updates","title":"TinyML Summit"},{"location":"conferences/tinymlsummit/#tinyml-summit","text":"Thank you for stopping by our Poster! Want to know more about ScaleDown?","title":"TinyML Summit"},{"location":"conferences/tinymlsummit/#check-out-a-short-demo","text":"Check out this short video and watch how ScaleDown can be implemented:","title":"Check out a Short Demo"},{"location":"conferences/tinymlsummit/#how-is-scaledown-helping-in-quicker-implementation","text":"ScaleDown makes it easy to train and deploy models by giving you wrappers over commonly used TinyML algorithms and techniques. ScaleDown also handles the implementation details and manages hardware-software interoperability making it easy to develop TinyML projects. Check out our how ScaleDown simplifies complex steps during a TinyML cycle.","title":"How is ScaleDown Helping in quicker implementation?"},{"location":"conferences/tinymlsummit/#stay-updated","text":"We are constantly looking to ramp our work and open-source package, sign up to get updates on new optimizations and framework adoptions. Sign Up for Updates","title":"Stay Updated"},{"location":"getting_started/User%20Guide/","text":"User Guide This is the user guide for Scaledown. It will walk over basic concepts used in TinyML which is used in the package. Quantization : Quantization is the process of converting larger floating bit values to integer values. This helps to do complex computations on resource- constraint devices. There are two types of quantization: Weight Quantization Activation Quantization Pruning : Neural Networks in their nature are overparametrised networks. With any given network, you can remove or prune some parts of it and still maintain the same metrics. But where do you begin pruning? Weights or Neurons? Pruning can be quantified and heuristically approached. To learn more, check out the guide on Pruning Knowledge Distillation : Knowledge Distillation as the word suggests, is the process of trickling down information from a larger(teacher) model to aa smaller(student) model. These student models can be deployed to microcontrollers. But how do you use such models? The idea comes from training a large models and learning parts of the information important to the data. And then deriving a smaller model which would learn from the output generated by the large model. This way, you don't overwhelm the student model and it is able to learn representions necessary to the dataset in a clear and concise manner. Check the guide to know more.","title":"User Guide"},{"location":"getting_started/User%20Guide/#user-guide","text":"This is the user guide for Scaledown. It will walk over basic concepts used in TinyML which is used in the package.","title":"User Guide"},{"location":"getting_started/User%20Guide/#quantization","text":"Quantization is the process of converting larger floating bit values to integer values. This helps to do complex computations on resource- constraint devices. There are two types of quantization: Weight Quantization Activation Quantization","title":"Quantization:"},{"location":"getting_started/User%20Guide/#pruning","text":"Neural Networks in their nature are overparametrised networks. With any given network, you can remove or prune some parts of it and still maintain the same metrics. But where do you begin pruning? Weights or Neurons? Pruning can be quantified and heuristically approached. To learn more, check out the guide on Pruning","title":"Pruning:"},{"location":"getting_started/User%20Guide/#knowledge-distillation","text":"Knowledge Distillation as the word suggests, is the process of trickling down information from a larger(teacher) model to aa smaller(student) model. These student models can be deployed to microcontrollers. But how do you use such models? The idea comes from training a large models and learning parts of the information important to the data. And then deriving a smaller model which would learn from the output generated by the large model. This way, you don't overwhelm the student model and it is able to learn representions necessary to the dataset in a clear and concise manner. Check the guide to know more.","title":"Knowledge Distillation:"},{"location":"getting_started/installation/","text":"Intro This getting started guide shows you how to use most of scaledown's features. Each page gradually introduces you to more advanced features and is divided into sections so you can use them as a reference when building your project. If instead you are looking for a more detailed API reference, you can find it here Install scaledown You can install scaledown by running pip install scaledown To get the latest version of scaledown, you can install the nightly build pip install sd-nightly Since scaledown is meant to be installed and used in edge devices, it has intentionally been made as lightweight as possible. No extra packages are included along with the package and any other framework or tool that you need to use should be installed separately. Learning Resources If you are new to TinyML and Edge Computing or if you need a refresher on some of its concepts, you can take a look at our learning resources","title":"Intro"},{"location":"getting_started/installation/#intro","text":"This getting started guide shows you how to use most of scaledown's features. Each page gradually introduces you to more advanced features and is divided into sections so you can use them as a reference when building your project. If instead you are looking for a more detailed API reference, you can find it here","title":"Intro"},{"location":"getting_started/installation/#install-scaledown","text":"You can install scaledown by running pip install scaledown To get the latest version of scaledown, you can install the nightly build pip install sd-nightly Since scaledown is meant to be installed and used in edge devices, it has intentionally been made as lightweight as possible. No extra packages are included along with the package and any other framework or tool that you need to use should be installed separately.","title":"Install scaledown"},{"location":"getting_started/installation/#learning-resources","text":"If you are new to TinyML and Edge Computing or if you need a refresher on some of its concepts, you can take a look at our learning resources","title":"Learning Resources"},{"location":"getting_started/knowledgedistillation/","text":"Knowledge Distillation image reference: Intel Labs Knowledge Distillation is a process of compressing information from a larger model to a smaller model. The larger model is trained on the dataset. This step is usually performed on high performing GPUs, so that we can make that the model learns all possible representations. How does this help? One of the popular steps to model training is to use an ensemble of models on the same data. This is then followed by averaging of predictions across all these models. But as we move towards dealing with larger datasets and adding more data in general, we need to shift to deeper neural networks for solving optimsations. Thus, it becomes computationally expensive to ensemble such models. Instead using a smaller model to understand the data and generalise over it, can benefit the process much better. So how do we achieve it? What we understand now is that, we want a larger or teacher model to transfer its ability of generalisation to a student model. Let us go through the process of neural network for simple classification, when a model predicts a class, it assigns a higher probabilities to that class and lower to the rest. This generates soft labels or targets. These soft labels are then distilled or transferred to the student or smaller model So when we train the student model on the same data, it is able to converge in a quicker manner and then produce the true labels needed for result. This student model has the advantage of being trained away from the teacher model, in any environment. This way it will still retain the soft labels, but won't be dependent on highly computationally devices for training. Thus making this system ideal for TinyML or Deep Learning in Microcontrollers. Does this really work? Now that we have understood the theory, let us look at some interesting results. The original paper on Knowledge Distillation covers the experiment on speech recognition. The authors use an ensemble of Deep Neural Networks that are used for Automatic Speech Recognition(ASR). The metrics used to udnerstand the results are Frame accuracy and Word Error Rate. The interesting takeway is that a single distilled model is able to gain the same accuracy as that 10 times an ensemble system Ok I am interested where do I start? Awesome, if we have got you interested, you can check the steps to Knowledge Distillation on Keras Alternatively, you can use the scaledown package and do it in less than 10 lines of code! Resources Towards Data Science: Knowledge Distillation Simplified Neural Network Distiller: Knowledge Distillation Research Paper: Distilling the Knowledge in a Neural Network","title":"Knowledge Distillation"},{"location":"getting_started/knowledgedistillation/#knowledge-distillation","text":"image reference: Intel Labs Knowledge Distillation is a process of compressing information from a larger model to a smaller model. The larger model is trained on the dataset. This step is usually performed on high performing GPUs, so that we can make that the model learns all possible representations.","title":"Knowledge Distillation"},{"location":"getting_started/knowledgedistillation/#how-does-this-help","text":"One of the popular steps to model training is to use an ensemble of models on the same data. This is then followed by averaging of predictions across all these models. But as we move towards dealing with larger datasets and adding more data in general, we need to shift to deeper neural networks for solving optimsations. Thus, it becomes computationally expensive to ensemble such models. Instead using a smaller model to understand the data and generalise over it, can benefit the process much better.","title":"How does this help?"},{"location":"getting_started/knowledgedistillation/#so-how-do-we-achieve-it","text":"What we understand now is that, we want a larger or teacher model to transfer its ability of generalisation to a student model. Let us go through the process of neural network for simple classification, when a model predicts a class, it assigns a higher probabilities to that class and lower to the rest. This generates soft labels or targets. These soft labels are then distilled or transferred to the student or smaller model So when we train the student model on the same data, it is able to converge in a quicker manner and then produce the true labels needed for result. This student model has the advantage of being trained away from the teacher model, in any environment. This way it will still retain the soft labels, but won't be dependent on highly computationally devices for training. Thus making this system ideal for TinyML or Deep Learning in Microcontrollers.","title":"So how do we achieve it?"},{"location":"getting_started/knowledgedistillation/#does-this-really-work","text":"Now that we have understood the theory, let us look at some interesting results. The original paper on Knowledge Distillation covers the experiment on speech recognition. The authors use an ensemble of Deep Neural Networks that are used for Automatic Speech Recognition(ASR). The metrics used to udnerstand the results are Frame accuracy and Word Error Rate. The interesting takeway is that a single distilled model is able to gain the same accuracy as that 10 times an ensemble system","title":"Does this really work?"},{"location":"getting_started/knowledgedistillation/#ok-i-am-interested-where-do-i-start","text":"Awesome, if we have got you interested, you can check the steps to Knowledge Distillation on Keras Alternatively, you can use the scaledown package and do it in less than 10 lines of code!","title":"Ok I am interested where do I start?"},{"location":"getting_started/knowledgedistillation/#resources","text":"Towards Data Science: Knowledge Distillation Simplified Neural Network Distiller: Knowledge Distillation Research Paper: Distilling the Knowledge in a Neural Network","title":"Resources"},{"location":"getting_started/pruning/","text":"Pruning image reference: TDS As we move towards using neural networks for lower constraint devices, one dilemma we constantly face is the size of deep neural networks. Since DNNs were conceputalised to deal with large data they inturn gave rise to larger models. These models grew larger over time, achieveing greater accuracy and also utilising enormous compute like high performance GPUs. Thus taking the focus from speed and putting to the performance based on accuracy. With TinyML, adopters quickly realised that such models are not built for real world systems, especially for deployment. Hence the easy and straightforward solution was to cut down or reduce the size of neural networks. So how do we cut down or prune neural networks? Neural networks are composed of different parameters: primarily, the weights and neurons. We know that if we reduce these parameters, it would eventually make the network smaller. But does this effect the model in any manner? Yes large neural networks, tend to be more accurate. If we were to prune some of it's parts, then neural networks would loose its accuracy. Hence, our goal is to make pruned networks loose less accuracy and preserve it as close to it's orginal model. This means, we need to systematically reduce parameters throughout the network. Methods of pruning networks As mentioned before one of the important parts of a neural network is the weights. Le us see, how do we prune weights in a neural network. 1. Magnitude weight based Pruning One easy way to reduce the weights is to set it to zero. But you don't want to set weights with higher values to zero. You would rather select weights close to zero and change them to zero. This is known as magnitude weigjht pruning which helps to trim insight weights in the network. 2. Neuron based Pruning Second way to prune is to make drop neurons from the network. But how do you do this without loosing too much of the accuracy? The first step is to understand the network and neurons present in it. Rank these neurons based on their importance and remove the least imprortant neuron In both the methods above, it is important to test the model and it's performance, and focus on doing it in an iterative process. Results: Does this really work? If we look at at Magntitude weight based Pruning, Tensorflow currently reports a 6x improvements in model compression with minimial loss in accuracy. They tested their model on Imagenet using InceptionV3 and MobileNetV1 based models: One can notice that even with 87.5% sparsity of networks, which means that 87.5% of the network was pruned, there is only a 2% drop in accuracy. Thus making these models perfect for low resource microcontrollers or compute devices. Ok I am interested where do I start? Awesome, if we have got you interested, you can check the steps to Pruning using Tensorflow Alternatively, you can use the scaledown package and do it in less than 10 lines of code! Resources Towards Data Science: Pruning Neural Networks Research Paper: WHAT IS THE STATE OF NEURAL NETWORK PRUNING? Wikipedia: Pruning(artificial neural networks)","title":"Pruning"},{"location":"getting_started/pruning/#pruning","text":"image reference: TDS As we move towards using neural networks for lower constraint devices, one dilemma we constantly face is the size of deep neural networks. Since DNNs were conceputalised to deal with large data they inturn gave rise to larger models. These models grew larger over time, achieveing greater accuracy and also utilising enormous compute like high performance GPUs. Thus taking the focus from speed and putting to the performance based on accuracy. With TinyML, adopters quickly realised that such models are not built for real world systems, especially for deployment. Hence the easy and straightforward solution was to cut down or reduce the size of neural networks.","title":"Pruning"},{"location":"getting_started/pruning/#so-how-do-we-cut-down-or-prune-neural-networks","text":"Neural networks are composed of different parameters: primarily, the weights and neurons. We know that if we reduce these parameters, it would eventually make the network smaller. But does this effect the model in any manner? Yes large neural networks, tend to be more accurate. If we were to prune some of it's parts, then neural networks would loose its accuracy. Hence, our goal is to make pruned networks loose less accuracy and preserve it as close to it's orginal model. This means, we need to systematically reduce parameters throughout the network.","title":"So how do we cut down or prune neural networks?"},{"location":"getting_started/pruning/#methods-of-pruning-networks","text":"As mentioned before one of the important parts of a neural network is the weights. Le us see, how do we prune weights in a neural network.","title":"Methods of pruning networks"},{"location":"getting_started/pruning/#1-magnitude-weight-based-pruning","text":"One easy way to reduce the weights is to set it to zero. But you don't want to set weights with higher values to zero. You would rather select weights close to zero and change them to zero. This is known as magnitude weigjht pruning which helps to trim insight weights in the network.","title":"1. Magnitude weight based Pruning"},{"location":"getting_started/pruning/#2-neuron-based-pruning","text":"Second way to prune is to make drop neurons from the network. But how do you do this without loosing too much of the accuracy? The first step is to understand the network and neurons present in it. Rank these neurons based on their importance and remove the least imprortant neuron In both the methods above, it is important to test the model and it's performance, and focus on doing it in an iterative process.","title":"2. Neuron based Pruning"},{"location":"getting_started/pruning/#results-does-this-really-work","text":"If we look at at Magntitude weight based Pruning, Tensorflow currently reports a 6x improvements in model compression with minimial loss in accuracy. They tested their model on Imagenet using InceptionV3 and MobileNetV1 based models: One can notice that even with 87.5% sparsity of networks, which means that 87.5% of the network was pruned, there is only a 2% drop in accuracy. Thus making these models perfect for low resource microcontrollers or compute devices.","title":"Results: Does this really work?"},{"location":"getting_started/pruning/#ok-i-am-interested-where-do-i-start","text":"Awesome, if we have got you interested, you can check the steps to Pruning using Tensorflow Alternatively, you can use the scaledown package and do it in less than 10 lines of code!","title":"Ok I am interested where do I start?"},{"location":"getting_started/pruning/#resources","text":"Towards Data Science: Pruning Neural Networks Research Paper: WHAT IS THE STATE OF NEURAL NETWORK PRUNING? Wikipedia: Pruning(artificial neural networks)","title":"Resources"},{"location":"getting_started/quantization/","text":"Quantization Definition of Quantization: When we look at signal processing, quantization orginally means the process of mapping input values to a large set of output values in a smaller set, with a finite number of elements. But as we look at edge computing Quantization is the process of reducing higher floating bit values like FP-32, FP-64 to lower bit precision values like int8 while making sure we preserve the information in the neural networks But why do we need Quantization? Need for Quantization When we look at Deep Neural Networks, they are often tained in full precision on Graphic Processing Units(GPUs). Why did we shift to GPUs? Our general purpose CPUs are based on Von Neumann digital hardware architecture. This severly limits a neural network's performance since we cannot perform Multiple and Accumulate operations. Hence, we shifted to GPUs which took advantage of parallelised architecture to perform these MAC operations on a large scale. But what happens when we shift to smaller devices? How do they differ from the large scale GPUs? Power Constraints: Traditionally Neural Networks require massive amount of power and energy while running on GPUs. A GPU on an average consumes 140WTDP. But smaller devices like microcontrollers and microprocessors, for example Raspberry Pi consumes 1-2W of power Memory Constraints: Most PCs come with a large memory and RAM; 4/8/16 GB is found on average in most devices. But when we look at edge device they have very less memory. Composed of Floating point: Traditionally Neural Networks contain floating point values, but as we move towards low powered devices, it is important to compress these large networks to run on them. One of the effective ways to do so, is to map the higher bit to lower precision bit values aka quantization Types of Quantization Weight Quantization: When we perform weight quantization, we convert all weight values to a lower precision like INT8. This reduces the size of the model. However when we execute the model, we need to convert our weights back to their full precision. So while we do get a lot of memory savings, there is no latency savings and there is a tiny overhead of converting up-converting all weight values. Weight and Activation Quantization: Instead of up-converting quantized weights when we perform activation quntization, we keep the activation in low precision and execute the model. By restricting all computation to INT8 precision, we can decrease computation latency as well as model size.","title":"Quantization"},{"location":"getting_started/quantization/#quantization","text":"","title":"Quantization"},{"location":"getting_started/quantization/#definition-of-quantization","text":"When we look at signal processing, quantization orginally means the process of mapping input values to a large set of output values in a smaller set, with a finite number of elements. But as we look at edge computing Quantization is the process of reducing higher floating bit values like FP-32, FP-64 to lower bit precision values like int8 while making sure we preserve the information in the neural networks But why do we need Quantization?","title":"Definition of Quantization:"},{"location":"getting_started/quantization/#need-for-quantization","text":"When we look at Deep Neural Networks, they are often tained in full precision on Graphic Processing Units(GPUs). Why did we shift to GPUs? Our general purpose CPUs are based on Von Neumann digital hardware architecture. This severly limits a neural network's performance since we cannot perform Multiple and Accumulate operations. Hence, we shifted to GPUs which took advantage of parallelised architecture to perform these MAC operations on a large scale. But what happens when we shift to smaller devices? How do they differ from the large scale GPUs? Power Constraints: Traditionally Neural Networks require massive amount of power and energy while running on GPUs. A GPU on an average consumes 140WTDP. But smaller devices like microcontrollers and microprocessors, for example Raspberry Pi consumes 1-2W of power Memory Constraints: Most PCs come with a large memory and RAM; 4/8/16 GB is found on average in most devices. But when we look at edge device they have very less memory. Composed of Floating point: Traditionally Neural Networks contain floating point values, but as we move towards low powered devices, it is important to compress these large networks to run on them. One of the effective ways to do so, is to map the higher bit to lower precision bit values aka quantization","title":"Need for Quantization"},{"location":"getting_started/quantization/#types-of-quantization","text":"","title":"Types of Quantization"},{"location":"getting_started/quantization/#weight-quantization","text":"When we perform weight quantization, we convert all weight values to a lower precision like INT8. This reduces the size of the model. However when we execute the model, we need to convert our weights back to their full precision. So while we do get a lot of memory savings, there is no latency savings and there is a tiny overhead of converting up-converting all weight values.","title":"Weight Quantization:"},{"location":"getting_started/quantization/#weight-and-activation-quantization","text":"Instead of up-converting quantized weights when we perform activation quntization, we keep the activation in low precision and execute the model. By restricting all computation to INT8 precision, we can decrease computation latency as well as model size.","title":"Weight and Activation Quantization:"},{"location":"guide/","text":"User Guide This is the user guide for Scaledown. It will walk over basic concepts used in TinyML which is used in the package. Quantization : Quantization is the process of converting larger floating bit values to integer values. This helps to do complex computations on resource- constraint devices. There are two types of quantization: Weight Quantization Activation Quantization Pruning : Neural Networks in their nature are overparametrised networks. With any given network, you can remove or prune some parts of it and still maintain the same metrics. But where do you begin pruning? Weights or Neurons? Pruning can be quantified and heuristically approached. To learn more, check out the guide on Pruning Knowledge Distillation : Knowledge Distillation as the word suggests, is the process of trickling down information from a larger(teacher) model to aa smaller(student) model. These student models can be deployed to microcontrollers. But how do you use such models? The idea comes from training a large models and learning parts of the information important to the data. And then deriving a smaller model which would learn from the output generated by the large model. This way, you don't overwhelm the student model and it is able to learn representions necessary to the dataset in a clear and concise manner. Check the guide to know more.","title":"Index"},{"location":"guide/#user-guide","text":"This is the user guide for Scaledown. It will walk over basic concepts used in TinyML which is used in the package.","title":"User Guide"},{"location":"guide/#quantization","text":"Quantization is the process of converting larger floating bit values to integer values. This helps to do complex computations on resource- constraint devices. There are two types of quantization: Weight Quantization Activation Quantization","title":"Quantization:"},{"location":"guide/#pruning","text":"Neural Networks in their nature are overparametrised networks. With any given network, you can remove or prune some parts of it and still maintain the same metrics. But where do you begin pruning? Weights or Neurons? Pruning can be quantified and heuristically approached. To learn more, check out the guide on Pruning","title":"Pruning:"},{"location":"guide/#knowledge-distillation","text":"Knowledge Distillation as the word suggests, is the process of trickling down information from a larger(teacher) model to aa smaller(student) model. These student models can be deployed to microcontrollers. But how do you use such models? The idea comes from training a large models and learning parts of the information important to the data. And then deriving a smaller model which would learn from the output generated by the large model. This way, you don't overwhelm the student model and it is able to learn representions necessary to the dataset in a clear and concise manner. Check the guide to know more.","title":"Knowledge Distillation:"},{"location":"guide/knowledgedistillation/","text":"Knowledge Distillation image reference: Intel Labs Knowledge Distillation is a process of compressing information from a larger model to a smaller model. The larger model is trained on the dataset. This step is usually performed on high performing GPUs, so that we can make that the model learns all possible representations. How does this help? One of the popular steps to model training is to use an ensemble of models on the same data. This is then followed by averaging of predictions across all these models. But as we move towards dealing with larger datasets and adding more data in general, we need to shift to deeper neural networks for solving optimsations. Thus, it becomes computationally expensive to ensemble such models. Instead using a smaller model to understand the data and generalise over it, can benefit the process much better. So how do we achieve it? What we understand now is that, we want a larger or teacher model to transfer its ability of generalisation to a student model. Let us go through the process of neural network for simple classification, when a model predicts a class, it assigns a higher probabilities to that class and lower to the rest. This generates soft labels or targets. These soft labels are then distilled or transferred to the student or smaller model So when we train the student model on the same data, it is able to converge in a quicker manner and then produce the true labels needed for result. This student model has the advantage of being trained away from the teacher model, in any environment. This way it will still retain the soft labels, but won't be dependent on highly computationally devices for training. Thus making this system ideal for TinyML or Deep Learning in Microcontrollers. Does this really work? Now that we have understood the theory, let us look at some interesting results. The original paper on Knowledge Distillation covers the experiment on speech recognition. The authors use an ensemble of Deep Neural Networks that are used for Automatic Speech Recognition(ASR). The metrics used to udnerstand the results are Frame accuracy and Word Error Rate. The interesting takeway is that a single distilled model is able to gain the same accuracy as that 10 times an ensemble system Ok I am interested where do I start? Awesome, if we have got you interested, you can check the steps to Knowledge Distillation on Keras Alternatively, you can use the scaledown package and do it in less than 10 lines of code! Resources Towards Data Science: Knowledge Distillation Simplified Neural Network Distiller: Knowledge Distillation Research Paper: Distilling the Knowledge in a Neural Network","title":"Knowledge Distillation"},{"location":"guide/knowledgedistillation/#knowledge-distillation","text":"image reference: Intel Labs Knowledge Distillation is a process of compressing information from a larger model to a smaller model. The larger model is trained on the dataset. This step is usually performed on high performing GPUs, so that we can make that the model learns all possible representations.","title":"Knowledge Distillation"},{"location":"guide/knowledgedistillation/#how-does-this-help","text":"One of the popular steps to model training is to use an ensemble of models on the same data. This is then followed by averaging of predictions across all these models. But as we move towards dealing with larger datasets and adding more data in general, we need to shift to deeper neural networks for solving optimsations. Thus, it becomes computationally expensive to ensemble such models. Instead using a smaller model to understand the data and generalise over it, can benefit the process much better.","title":"How does this help?"},{"location":"guide/knowledgedistillation/#so-how-do-we-achieve-it","text":"What we understand now is that, we want a larger or teacher model to transfer its ability of generalisation to a student model. Let us go through the process of neural network for simple classification, when a model predicts a class, it assigns a higher probabilities to that class and lower to the rest. This generates soft labels or targets. These soft labels are then distilled or transferred to the student or smaller model So when we train the student model on the same data, it is able to converge in a quicker manner and then produce the true labels needed for result. This student model has the advantage of being trained away from the teacher model, in any environment. This way it will still retain the soft labels, but won't be dependent on highly computationally devices for training. Thus making this system ideal for TinyML or Deep Learning in Microcontrollers.","title":"So how do we achieve it?"},{"location":"guide/knowledgedistillation/#does-this-really-work","text":"Now that we have understood the theory, let us look at some interesting results. The original paper on Knowledge Distillation covers the experiment on speech recognition. The authors use an ensemble of Deep Neural Networks that are used for Automatic Speech Recognition(ASR). The metrics used to udnerstand the results are Frame accuracy and Word Error Rate. The interesting takeway is that a single distilled model is able to gain the same accuracy as that 10 times an ensemble system","title":"Does this really work?"},{"location":"guide/knowledgedistillation/#ok-i-am-interested-where-do-i-start","text":"Awesome, if we have got you interested, you can check the steps to Knowledge Distillation on Keras Alternatively, you can use the scaledown package and do it in less than 10 lines of code!","title":"Ok I am interested where do I start?"},{"location":"guide/knowledgedistillation/#resources","text":"Towards Data Science: Knowledge Distillation Simplified Neural Network Distiller: Knowledge Distillation Research Paper: Distilling the Knowledge in a Neural Network","title":"Resources"},{"location":"guide/pruning/","text":"Pruning image reference: TDS As we move towards using neural networks for lower constraint devices, one dilemma we constantly face is the size of deep neural networks. Since DNNs were conceputalised to deal with large data they inturn gave rise to larger models. These models grew larger over time, achieveing greater accuracy and also utilising enormous compute like high performance GPUs. Thus taking the focus from speed and putting to the performance based on accuracy. With TinyML, adopters quickly realised that such models are not built for real world systems, especially for deployment. Hence the easy and straightforward solution was to cut down or reduce the size of neural networks. So how do we cut down or prune neural networks? Neural networks are composed of different parameters: primarily, the weights and neurons. We know that if we reduce these parameters, it would eventually make the network smaller. But does this effect the model in any manner? Yes large neural networks, tend to be more accurate. If we were to prune some of it's parts, then neural networks would loose its accuracy. Hence, our goal is to make pruned networks loose less accuracy and preserve it as close to it's orginal model. This means, we need to systematically reduce parameters throughout the network. Methods of pruning networks As mentioned before one of the important parts of a neural network is the weights. Le us see, how do we prune weights in a neural network. 1. Magnitude weight based Pruning One easy way to reduce the weights is to set it to zero. But you don't want to set weights with higher values to zero. You would rather select weights close to zero and change them to zero. This is known as magnitude weigjht pruning which helps to trim insight weights in the network. 2. Neuron based Pruning Second way to prune is to make drop neurons from the network. But how do you do this without loosing too much of the accuracy? The first step is to understand the network and neurons present in it. Rank these neurons based on their importance and remove the least imprortant neuron In both the methods above, it is important to test the model and it's performance, and focus on doing it in an iterative process. Results: Does this really work? If we look at at Magntitude weight based Pruning, Tensorflow currently reports a 6x improvements in model compression with minimial loss in accuracy. They tested their model on Imagenet using InceptionV3 and MobileNetV1 based models: One can notice that even with 87.5% sparsity of networks, which means that 87.5% of the network was pruned, there is only a 2% drop in accuracy. Thus making these models perfect for low resource microcontrollers or compute devices. Ok I am interested where do I start? Awesome, if we have got you interested, you can check the steps to Pruning using Tensorflow Alternatively, you can use the scaledown package and do it in less than 10 lines of code! Resources Towards Data Science: Pruning Neural Networks Research Paper: WHAT IS THE STATE OF NEURAL NETWORK PRUNING? Wikipedia: Pruning(artificial neural networks)","title":"Pruning"},{"location":"guide/pruning/#pruning","text":"image reference: TDS As we move towards using neural networks for lower constraint devices, one dilemma we constantly face is the size of deep neural networks. Since DNNs were conceputalised to deal with large data they inturn gave rise to larger models. These models grew larger over time, achieveing greater accuracy and also utilising enormous compute like high performance GPUs. Thus taking the focus from speed and putting to the performance based on accuracy. With TinyML, adopters quickly realised that such models are not built for real world systems, especially for deployment. Hence the easy and straightforward solution was to cut down or reduce the size of neural networks.","title":"Pruning"},{"location":"guide/pruning/#so-how-do-we-cut-down-or-prune-neural-networks","text":"Neural networks are composed of different parameters: primarily, the weights and neurons. We know that if we reduce these parameters, it would eventually make the network smaller. But does this effect the model in any manner? Yes large neural networks, tend to be more accurate. If we were to prune some of it's parts, then neural networks would loose its accuracy. Hence, our goal is to make pruned networks loose less accuracy and preserve it as close to it's orginal model. This means, we need to systematically reduce parameters throughout the network.","title":"So how do we cut down or prune neural networks?"},{"location":"guide/pruning/#methods-of-pruning-networks","text":"As mentioned before one of the important parts of a neural network is the weights. Le us see, how do we prune weights in a neural network.","title":"Methods of pruning networks"},{"location":"guide/pruning/#1-magnitude-weight-based-pruning","text":"One easy way to reduce the weights is to set it to zero. But you don't want to set weights with higher values to zero. You would rather select weights close to zero and change them to zero. This is known as magnitude weigjht pruning which helps to trim insight weights in the network.","title":"1. Magnitude weight based Pruning"},{"location":"guide/pruning/#2-neuron-based-pruning","text":"Second way to prune is to make drop neurons from the network. But how do you do this without loosing too much of the accuracy? The first step is to understand the network and neurons present in it. Rank these neurons based on their importance and remove the least imprortant neuron In both the methods above, it is important to test the model and it's performance, and focus on doing it in an iterative process.","title":"2. Neuron based Pruning"},{"location":"guide/pruning/#results-does-this-really-work","text":"If we look at at Magntitude weight based Pruning, Tensorflow currently reports a 6x improvements in model compression with minimial loss in accuracy. They tested their model on Imagenet using InceptionV3 and MobileNetV1 based models: One can notice that even with 87.5% sparsity of networks, which means that 87.5% of the network was pruned, there is only a 2% drop in accuracy. Thus making these models perfect for low resource microcontrollers or compute devices.","title":"Results: Does this really work?"},{"location":"guide/pruning/#ok-i-am-interested-where-do-i-start","text":"Awesome, if we have got you interested, you can check the steps to Pruning using Tensorflow Alternatively, you can use the scaledown package and do it in less than 10 lines of code!","title":"Ok I am interested where do I start?"},{"location":"guide/pruning/#resources","text":"Towards Data Science: Pruning Neural Networks Research Paper: WHAT IS THE STATE OF NEURAL NETWORK PRUNING? Wikipedia: Pruning(artificial neural networks)","title":"Resources"},{"location":"guide/quantization/","text":"Quantization Definition of Quantization: When we look at signal processing, quantization orginally means the process of mapping input values to a large set of output values in a smaller set, with a finite number of elements. But as we look at edge computing Quantization is the process of reducing higher floating bit values like FP-32, FP-64 to lower bit precision values like int8 while making sure we preserve the information in the neural networks But why do we need Quantization? Need for Quantization When we look at Deep Neural Networks, they are often tained in full precision on Graphic Processing Units(GPUs). Why did we shift to GPUs? Our general purpose CPUs are based on Von Neumann digital hardware architecture. This severly limits a neural network's performance since we cannot perform Multiple and Accumulate operations. Hence, we shifted to GPUs which took advantage of parallelised architecture to perform these MAC operations on a large scale. But what happens when we shift to smaller devices? How do they differ from the large scale GPUs? Power Constraints: Traditionally Neural Networks require massive amount of power and energy while running on GPUs. A GPU on an average consumes 140WTDP. But smaller devices like microcontrollers and microprocessors, for example Raspberry Pi consumes 1-2W of power Memory Constraints: Most PCs come with a large memory and RAM; 4/8/16 GB is found on average in most devices. But when we look at edge device they have very less memory. Composed of Floating point: Traditionally Neural Networks contain floating point values, but as we move towards low powered devices, it is important to compress these large networks to run on them. One of the effective ways to do so, is to map the higher bit to lower precision bit values aka quantization Types of Quantization Weight Quantization: When we perform weight quantization, we convert all weight values to a lower precision like INT8. This reduces the size of the model. However when we execute the model, we need to convert our weights back to their full precision. So while we do get a lot of memory savings, there is no latency savings and there is a tiny overhead of converting up-converting all weight values. Weight and Activation Quantization: Instead of up-converting quantized weights when we perform activation quntization, we keep the activation in low precision and execute the model. By restricting all computation to INT8 precision, we can decrease computation latency as well as model size.","title":"Quantization"},{"location":"guide/quantization/#quantization","text":"","title":"Quantization"},{"location":"guide/quantization/#definition-of-quantization","text":"When we look at signal processing, quantization orginally means the process of mapping input values to a large set of output values in a smaller set, with a finite number of elements. But as we look at edge computing Quantization is the process of reducing higher floating bit values like FP-32, FP-64 to lower bit precision values like int8 while making sure we preserve the information in the neural networks But why do we need Quantization?","title":"Definition of Quantization:"},{"location":"guide/quantization/#need-for-quantization","text":"When we look at Deep Neural Networks, they are often tained in full precision on Graphic Processing Units(GPUs). Why did we shift to GPUs? Our general purpose CPUs are based on Von Neumann digital hardware architecture. This severly limits a neural network's performance since we cannot perform Multiple and Accumulate operations. Hence, we shifted to GPUs which took advantage of parallelised architecture to perform these MAC operations on a large scale. But what happens when we shift to smaller devices? How do they differ from the large scale GPUs? Power Constraints: Traditionally Neural Networks require massive amount of power and energy while running on GPUs. A GPU on an average consumes 140WTDP. But smaller devices like microcontrollers and microprocessors, for example Raspberry Pi consumes 1-2W of power Memory Constraints: Most PCs come with a large memory and RAM; 4/8/16 GB is found on average in most devices. But when we look at edge device they have very less memory. Composed of Floating point: Traditionally Neural Networks contain floating point values, but as we move towards low powered devices, it is important to compress these large networks to run on them. One of the effective ways to do so, is to map the higher bit to lower precision bit values aka quantization","title":"Need for Quantization"},{"location":"guide/quantization/#types-of-quantization","text":"","title":"Types of Quantization"},{"location":"guide/quantization/#weight-quantization","text":"When we perform weight quantization, we convert all weight values to a lower precision like INT8. This reduces the size of the model. However when we execute the model, we need to convert our weights back to their full precision. So while we do get a lot of memory savings, there is no latency savings and there is a tiny overhead of converting up-converting all weight values.","title":"Weight Quantization:"},{"location":"guide/quantization/#weight-and-activation-quantization","text":"Instead of up-converting quantized weights when we perform activation quntization, we keep the activation in low precision and execute the model. By restricting all computation to INT8 precision, we can decrease computation latency as well as model size.","title":"Weight and Activation Quantization:"},{"location":"hardware_library/donatetous/","text":"Donate Donate Hardware \"Did you know that Hardware Devices cost almost 2-3 times a average daily income in developing countries?\" We are currently supporting our community by \"Hardware Library\" which provides indviduals with a chance to try out TinyML without the need of investing in New Hardware. You can help us by donating your hardware to the library. Reach out to us for donating hardware! Collaborate with Us Learning and Development are present in our core model as we strive to achieve more people actively using TinyML and increase the ease of deployment. Join us in this journey! Contribute to our framework Use our Libarary and Framework","title":"Donate"},{"location":"hardware_library/donatetous/#donate","text":"","title":"Donate"},{"location":"hardware_library/donatetous/#donate-hardware","text":"\"Did you know that Hardware Devices cost almost 2-3 times a average daily income in developing countries?\" We are currently supporting our community by \"Hardware Library\" which provides indviduals with a chance to try out TinyML without the need of investing in New Hardware. You can help us by donating your hardware to the library. Reach out to us for donating hardware!","title":"Donate Hardware"},{"location":"hardware_library/donatetous/#collaborate-with-us","text":"Learning and Development are present in our core model as we strive to achieve more people actively using TinyML and increase the ease of deployment. Join us in this journey!","title":"Collaborate with Us"},{"location":"hardware_library/donatetous/#contribute-to-our-framework","text":"","title":"Contribute to our framework"},{"location":"hardware_library/donatetous/#use-our-libarary-and-framework","text":"","title":"Use our Libarary and Framework"},{"location":"hardware_library/hardwarepresent/","text":"Hardwares Present Microprocessors Raspberry Pi 3B+ Raspberry Pi 4 Intel Atom Board The Raspberry Pi 3 Model B+ is the final revision in the Raspberry Pi 3 range. * Broadcom BCM2837B0, Cortex-A53 (ARMv8) 64-bit SoC @ 1.4GHz * 1GB LPDDR2 SDRAM * 2.4GHz and 5GHz IEEE 802.11.b/g/n/ac wireless LAN, Bluetooth 4.2, BLE * Gigabit Ethernet over USB 2.0 (maximum throughput 300 Mbps) * Extended 40-pin GPIO header * Full-size HDMI * 4 USB 2.0 ports * CSI camera port for connecting a Raspberry Pi camera * DSI display port for connecting a Raspberry Pi touchscreen display * 4-pole stereo output and composite video port * Micro SD port for loading your operating system and storing data * 5V/2.5A DC power input * Power-over-Ethernet (PoE) support (requires separate PoE HAT) This is the latest model of Raspberry Pi * Broadcom BCM2711, Quad core Cortex-A72 (ARM v8) 64-bit SoC @ 1.5GHz * 2GB, 4GB or 8GB LPDDR4-3200 SDRAM (depending on model) * 2.4 GHz and 5.0 GHz IEEE 802.11ac wireless, Bluetooth 5.0, BLE Gigabit Ethernet * 2 USB 3.0 ports; 2 USB 2.0 ports. * 2 \u00d7 micro-HDMI ports (up to 4kp60 supported) * 2-lane MIPI DSI display port * 2-lane MIPI CSI camera port * 4-pole stereo audio and composite video port * H.265 (4kp60 decode), H264 (1080p60 decode, 1080p30 encode) * OpenGL ES 3.1, Vulkan 1.0 * Micro-SD card slot for loading operating system and data storage * 5V DC via USB-C connector (minimum 3A*) * A good quality 2.5A power supply can be used if downstream USB peripherals consume less than 500mA in total. The Atom x5-Z8350 is a 64-bit quad-core system on a chip introduce by Intel in early 2015 * This ultra-low power SoC has a scenario design power of 2 W * Operates at a base frequency of 1.44 GHz with a burst up to 1.92 GHz * Supports up to 2 GB of memory * This chip incorporates the HD Graphics (Cherry Trail) GPU * Four planes available per pipe - 1x Primary, 2x Video Sprite & 1x Cursor. * Two dedicated digital Display Serial Interface PHYs implementing MIPI-DSI support. Microcontrollers Raspberry Pi Zero Arduino Nano rp2040 Nordic Thingy 91 Arduino Nano 33 BLE Sense The Canakit Raspberry Pi Zero is ultra-small, ultra-slim, and the smallest form factor Raspberry Pi with Wi-Fi and Bluetooth connectivity on board. * 1GHz single-core CPU * 512MB RAM * Mini HDMI port * Micro USB OTG port * Micro USB power * HAT-compatible 40-pin header * Composite video and reset headers * CSI camera connector (v1.3 only) Features * The only RP2040 board with connectivity * The only RP2040 board in the well-known and very popular * Nano form factor with castellated pads * 6-axis IMU, microphone, RGB LED, etc * Powerful Arduino board for makers * More RAM and dual-core, allowing new projects * Its IMU has machine learning capabilities, thus additional detection algorithms * Microphone to make voice-enabled projects * The ESP32 core on-board can be used as a co-processor The Nordic Thingy:91 is a battery-operated prototyping platform for cellular IoT,certified for global operation * Battery-operated prototyping platform for the nRF9160 SiP * nRF52840 board controller * LTE-M/NB-IoT/GNSS, Bluetooth LE and NFC antennas * User-programmable button and RGB LEDs * Environmental sensor for temperature,humidity, air quality and air pressure, plus a color and light sensor * Low-power accelerometer and high-g accelerometer * 4 x N-MOS transistors for external DC motors or LEDs * Rechargeable Li-Po battery with 1350 mAh capacity The Arduino Nano 33 BLE Sense is a completely new board on a well-known form factor. It comes with a series of embedded sensors: * The Nano 33 BLE Sense (without headers) is Arduino\u2019s 3.3V AI enabled board in the smallest available form factor: 45x18mm. * 9 axis inertial sensor: what makes this board ideal for wearable devices * humidity, and temperature sensor: to get highly accurate measurements of the environmental conditions * barometric sensor: you could make a simple weather station * microphone: to capture and analyse sound in real time * gesture, proximity, light color and light intensity sensor : estimate the room\u2019s luminosity, but also whether someone is moving close to the board","title":"Hardwares Present"},{"location":"hardware_library/hardwarepresent/#hardwares-present","text":"","title":"Hardwares Present"},{"location":"hardware_library/hardwarepresent/#microprocessors","text":"Raspberry Pi 3B+ Raspberry Pi 4 Intel Atom Board The Raspberry Pi 3 Model B+ is the final revision in the Raspberry Pi 3 range. * Broadcom BCM2837B0, Cortex-A53 (ARMv8) 64-bit SoC @ 1.4GHz * 1GB LPDDR2 SDRAM * 2.4GHz and 5GHz IEEE 802.11.b/g/n/ac wireless LAN, Bluetooth 4.2, BLE * Gigabit Ethernet over USB 2.0 (maximum throughput 300 Mbps) * Extended 40-pin GPIO header * Full-size HDMI * 4 USB 2.0 ports * CSI camera port for connecting a Raspberry Pi camera * DSI display port for connecting a Raspberry Pi touchscreen display * 4-pole stereo output and composite video port * Micro SD port for loading your operating system and storing data * 5V/2.5A DC power input * Power-over-Ethernet (PoE) support (requires separate PoE HAT) This is the latest model of Raspberry Pi * Broadcom BCM2711, Quad core Cortex-A72 (ARM v8) 64-bit SoC @ 1.5GHz * 2GB, 4GB or 8GB LPDDR4-3200 SDRAM (depending on model) * 2.4 GHz and 5.0 GHz IEEE 802.11ac wireless, Bluetooth 5.0, BLE Gigabit Ethernet * 2 USB 3.0 ports; 2 USB 2.0 ports. * 2 \u00d7 micro-HDMI ports (up to 4kp60 supported) * 2-lane MIPI DSI display port * 2-lane MIPI CSI camera port * 4-pole stereo audio and composite video port * H.265 (4kp60 decode), H264 (1080p60 decode, 1080p30 encode) * OpenGL ES 3.1, Vulkan 1.0 * Micro-SD card slot for loading operating system and data storage * 5V DC via USB-C connector (minimum 3A*) * A good quality 2.5A power supply can be used if downstream USB peripherals consume less than 500mA in total. The Atom x5-Z8350 is a 64-bit quad-core system on a chip introduce by Intel in early 2015 * This ultra-low power SoC has a scenario design power of 2 W * Operates at a base frequency of 1.44 GHz with a burst up to 1.92 GHz * Supports up to 2 GB of memory * This chip incorporates the HD Graphics (Cherry Trail) GPU * Four planes available per pipe - 1x Primary, 2x Video Sprite & 1x Cursor. * Two dedicated digital Display Serial Interface PHYs implementing MIPI-DSI support.","title":"Microprocessors"},{"location":"hardware_library/hardwarepresent/#microcontrollers","text":"Raspberry Pi Zero Arduino Nano rp2040 Nordic Thingy 91 Arduino Nano 33 BLE Sense The Canakit Raspberry Pi Zero is ultra-small, ultra-slim, and the smallest form factor Raspberry Pi with Wi-Fi and Bluetooth connectivity on board. * 1GHz single-core CPU * 512MB RAM * Mini HDMI port * Micro USB OTG port * Micro USB power * HAT-compatible 40-pin header * Composite video and reset headers * CSI camera connector (v1.3 only) Features * The only RP2040 board with connectivity * The only RP2040 board in the well-known and very popular * Nano form factor with castellated pads * 6-axis IMU, microphone, RGB LED, etc * Powerful Arduino board for makers * More RAM and dual-core, allowing new projects * Its IMU has machine learning capabilities, thus additional detection algorithms * Microphone to make voice-enabled projects * The ESP32 core on-board can be used as a co-processor The Nordic Thingy:91 is a battery-operated prototyping platform for cellular IoT,certified for global operation * Battery-operated prototyping platform for the nRF9160 SiP * nRF52840 board controller * LTE-M/NB-IoT/GNSS, Bluetooth LE and NFC antennas * User-programmable button and RGB LEDs * Environmental sensor for temperature,humidity, air quality and air pressure, plus a color and light sensor * Low-power accelerometer and high-g accelerometer * 4 x N-MOS transistors for external DC motors or LEDs * Rechargeable Li-Po battery with 1350 mAh capacity The Arduino Nano 33 BLE Sense is a completely new board on a well-known form factor. It comes with a series of embedded sensors: * The Nano 33 BLE Sense (without headers) is Arduino\u2019s 3.3V AI enabled board in the smallest available form factor: 45x18mm. * 9 axis inertial sensor: what makes this board ideal for wearable devices * humidity, and temperature sensor: to get highly accurate measurements of the environmental conditions * barometric sensor: you could make a simple weather station * microphone: to capture and analyse sound in real time * gesture, proximity, light color and light intensity sensor : estimate the room\u2019s luminosity, but also whether someone is moving close to the board","title":"Microcontrollers"},{"location":"hardware_library/intro/","text":"Introduction The Hardware Library is a physical library present in a city near you. How does it work? The Hardware Library is like any other library, you can use the hardware and then return it. Anyone can use the hardware present in the list below, free of cost. Once they are done experimenting and running their first hardware library, they can return it and get another one, to experiment on. Currently we have started our library in Singapore. List of Hardware and Specs Check out the Current Hardware Present along with their specifications here Want to start a Hardware Library? Do you want to start a hardware library at your city. Reach out to us Frequently asked Questions How do I know which Hardware to pick? Check out our current Hardware present, go through the specifications and decide on which one suits your project or need How do I return my hardware to the library? We will send a detailed instruction on how to mail back the hardware. It will be mostly through normal postage services. Alternatively, it can be left at POP Station near you and will be collected by us. I loved the experience, how can I get more hardware? You can get more Hardware, by requesting for it. There is a section, where you can mention if you have borrowed hardware before. We also believe in giving the knowledge back to the community, so show us how you used the hardware and we would love to give you more! Can I place a request for 2-3 hardware devices? Currently our hardware devices are limited, so we will only be providing one at a time. Not to fret, we are constantly reaching out to indivduals and organisations to help us with the supply of devices I have some extra hardware devices, how can I contribute/donate? Thank you so much for donating hardware to our community. Your efforts will go a long way in democratising TinyML education in the community. You can fill up this form or reach out us , and we will get back to you with the logistics","title":"Introduction"},{"location":"hardware_library/intro/#introduction","text":"The Hardware Library is a physical library present in a city near you.","title":"Introduction"},{"location":"hardware_library/intro/#how-does-it-work","text":"The Hardware Library is like any other library, you can use the hardware and then return it. Anyone can use the hardware present in the list below, free of cost. Once they are done experimenting and running their first hardware library, they can return it and get another one, to experiment on. Currently we have started our library in Singapore.","title":"How does it work?"},{"location":"hardware_library/intro/#list-of-hardware-and-specs","text":"Check out the Current Hardware Present along with their specifications here","title":"List of Hardware and Specs"},{"location":"hardware_library/intro/#want-to-start-a-hardware-library","text":"Do you want to start a hardware library at your city. Reach out to us","title":"Want to start a Hardware Library?"},{"location":"hardware_library/intro/#frequently-asked-questions","text":"How do I know which Hardware to pick? Check out our current Hardware present, go through the specifications and decide on which one suits your project or need How do I return my hardware to the library? We will send a detailed instruction on how to mail back the hardware. It will be mostly through normal postage services. Alternatively, it can be left at POP Station near you and will be collected by us. I loved the experience, how can I get more hardware? You can get more Hardware, by requesting for it. There is a section, where you can mention if you have borrowed hardware before. We also believe in giving the knowledge back to the community, so show us how you used the hardware and we would love to give you more! Can I place a request for 2-3 hardware devices? Currently our hardware devices are limited, so we will only be providing one at a time. Not to fret, we are constantly reaching out to indivduals and organisations to help us with the supply of devices I have some extra hardware devices, how can I contribute/donate? Thank you so much for donating hardware to our community. Your efforts will go a long way in democratising TinyML education in the community. You can fill up this form or reach out us , and we will get back to you with the logistics","title":"Frequently asked Questions"},{"location":"hardware_library/requestforhardware/","text":"Request for Hardware Thank you so much for reaching out to us and taking this leap in your journey to learn about TinyML. We are excited to have you working and building towards your first project in TinyML. Please fill up this form and we will get back to you soon with details on how to procure your hardware. Note: Please note this initiative is currently only available in Singapore","title":"Request for Hardware"},{"location":"hardware_library/requestforhardware/#request-for-hardware","text":"Thank you so much for reaching out to us and taking this leap in your journey to learn about TinyML. We are excited to have you working and building towards your first project in TinyML. Please fill up this form and we will get back to you soon with details on how to procure your hardware. Note: Please note this initiative is currently only available in Singapore","title":"Request for Hardware"},{"location":"team/joinus/","text":"Join Us","title":"Join Us"},{"location":"team/joinus/#join-us","text":"","title":"Join Us"},{"location":"team/team/","text":"Team Soham Chatterjee Vaidheeswaran Archana Avyay Sah ML Engineer @Sleek, Instructor @Udacity Reach out via: Twitter | LinkedIn | Github Data Scientist @DHL Express, Data Product Manager @WomenWhoCode Reach out via: Twitter | LinkedIn | Github Graduate Student @Purdue University, Co-Founder @Walkity Reach out via: LinkedIn Join Us \"Are you excited as we are about democratizing TinyML education? Then Join Us to build a framework or set up a hardware library near you or even create educational resources for the community!\"","title":"Scaledown Team"},{"location":"team/team/#team","text":"Soham Chatterjee Vaidheeswaran Archana Avyay Sah ML Engineer @Sleek, Instructor @Udacity Reach out via: Twitter | LinkedIn | Github Data Scientist @DHL Express, Data Product Manager @WomenWhoCode Reach out via: Twitter | LinkedIn | Github Graduate Student @Purdue University, Co-Founder @Walkity Reach out via: LinkedIn","title":"Team"},{"location":"team/team/#join-us","text":"\"Are you excited as we are about democratizing TinyML education? Then Join Us to build a framework or set up a hardware library near you or even create educational resources for the community!\"","title":"Join Us"}]}